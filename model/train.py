import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import os

print("=" * 60)
print("ğŸŒ± Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… (MNIST)")
print("=" * 60)

# 1. ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª MNIST
print("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

# 2. Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ÙˆØªØ·Ø¨ÙŠØ¹
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255.0
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255.0

print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(train_images)} ØµÙˆØ±Ø© ØªØ¯Ø±ÙŠØ¨ Ùˆ {len(test_images)} ØµÙˆØ±Ø© Ø§Ø®ØªØ¨Ø§Ø±")
print(f"ğŸ“ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØµÙˆØ±: {train_images.shape[1:]} (Ø§Ø±ØªÙØ§Ø¹Ã—Ø¹Ø±Ø¶Ã—Ù‚Ù†ÙˆØ§Øª)")

# 3. ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙØ¦Ø§Øª
class_names = ['ØµÙØ±', 'ÙˆØ§Ø­Ø¯', 'Ø§Ø«Ù†Ø§Ù†', 'Ø«Ù„Ø§Ø«Ø©', 'Ø£Ø±Ø¨Ø¹Ø©', 
               'Ø®Ù…Ø³Ø©', 'Ø³ØªØ©', 'Ø³Ø¨Ø¹Ø©', 'Ø«Ù…Ø§Ù†ÙŠØ©', 'ØªØ³Ø¹Ø©']
print(f"ğŸ”¤ Ø§Ù„ÙØ¦Ø§Øª: {class_names}")

# 4. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Dataset
BATCH_SIZE = 32
train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
train_ds = train_ds.shuffle(60000).batch(BATCH_SIZE)

val_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))
val_ds = val_ds.batch(BATCH_SIZE)

print(f"ğŸ“Š Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©: {BATCH_SIZE} ØµÙˆØ±Ø©")

# 5. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ - Ù…Ù‡Ù…: Ù„ØµÙˆØ± MNIST (28,28,1)
print("\nğŸ§  Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©...")
model = models.Sequential([
    # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù‡Ù…Ø©: input_shape ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† (28, 28, 1)
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')  # 10 ÙØ¦Ø§Øª Ù„Ù„Ø£Ø±Ù‚Ø§Ù… 0-9
])

# 6. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 7. Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print("\nğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
model.summary()

# 8. Ø§Ù„ØªØ¯Ø±ÙŠØ¨
EPOCHS = 5  # Ù‚Ù„Ù„Ù†Ø§ Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
print(f"\nğŸ”¥ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ({EPOCHS} Ø¯ÙˆØ±Ø§Øª)...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS
)

# 9. Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
print("\nğŸ“ˆ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±...")
test_loss, test_acc = model.evaluate(val_ds)
print(f"âœ… Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {test_acc:.4f} ({test_acc*100:.1f}%)")

# 10. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model_path = 'mnist_model.keras'
model.save(model_path)
print(f"ğŸ’¾ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸ ÙÙŠ: {model_path}")

# 11. Ø§Ø®ØªØ¨Ø§Ø± ØªÙ†Ø¨Ø¤ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
print("\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± ØªÙ†Ø¨Ø¤ Ø¹Ø´ÙˆØ§Ø¦ÙŠ...")
sample_image = test_images[0:1]  # Ø£ÙˆÙ„ ØµÙˆØ±Ø© Ø§Ø®ØªØ¨Ø§Ø±
predictions = model.predict(sample_image)
predicted_class = np.argmax(predictions[0])
confidence = np.max(predictions[0])
print(f"ğŸ“¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
print(f"   - Ø§Ù„ØªÙ†Ø¨Ø¤: {class_names[predicted_class]}")
print(f"   - Ø§Ù„Ø«Ù‚Ø©: {confidence*100:.1f}%")
print(f"   - Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ: {class_names[test_labels[0]]}")

print("\n" + "=" * 60)
print("ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
print("=" * 60)